{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "csv headings: id, created_at, source, original_text, clean_text, favorite_count, retweet_count, hashtags, trend <br>\n",
    "hashtags format: strings with comma separated hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File paths\n",
    "US_tweets_file = './Data/USTweets.csv'\n",
    "UK_tweets_file = './Data/UKTweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "tweets = []\n",
    "hashtags = [] #list of lists of hashtags e.g. hashtags[0] = [\"hashtag1\", \"hashtag2\"]\n",
    "hashtags_strings = [] #list of hashtags string e.g. hashtags[0] = [\"hashtag1, hashtag2\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(UK_tweets_file) as data_file:\n",
    "    data = csv.reader(data_file)\n",
    "    next(data) #To skip the headings\n",
    "    for row in data:\n",
    "        tweets.append(row[4])\n",
    "        hashtags.append(row[7].split(\", \"))\n",
    "        hashtags_strings.append(row[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(US_tweets_file) as data_file:\n",
    "#     data = csv.reader(data_file)\n",
    "#     next(data) #To skip the headings\n",
    "#     for row in data:\n",
    "#         tweets.append(row[4])\n",
    "#         hashtags.append(row[7].split(\", \"))\n",
    "#         hashtags_strings.append(row[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tokenizers\n",
    "Will use a specialized tokenizer for the hashtags because we need to encode all the hashtags. It also does not matter if the encoding of the tweets match the encoding of the hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tweets_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tweets_tokenizer.fit_on_texts(tweets)\n",
    "tweets_word_index = tweets_tokenizer.word_index\n",
    "tweets_index_word = tweets_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "hashtags_tokenizer.fit_on_texts(hashtags_strings)\n",
    "hashtags_word_index = hashtags_tokenizer.word_index\n",
    "hashtags_index_word = hashtags_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(tweets)} tweets, ')\n",
    "print(f'the tweets contain {len(tweets_tokenizer.word_index)} different words.')\n",
    "print(f'There are {len(hashtags_tokenizer.word_index)} different hashtags')\n",
    "print('Here are the tokenized hashtags')\n",
    "print(hashtags_word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "num_tokens = len(tweets_word_index) + 1\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "with open('../Hashtag-recomender-embeddings/embeddings_index_object.pkl', 'rb') as embeddings_file:\n",
    "    embeddings_index = pickle.load(embeddings_file)\n",
    "    for word, i in tweets_word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            if embedding_vector.shape == (300,):\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits:8633, misses: 1734\n"
     ]
    }
   ],
   "source": [
    "print(f'hits:{hits}, misses: {misses}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the sequences and pad them and one multi-hot encode the hashtags\n",
    "Will use a binary vector to encode the hashtags to the model can categorize the tweets. e.g. hashtags[0] = [tag1, tag2], and tag1 has encoding of 1 and tag2 has encoding 2, then the binary vector wil be [0 1 1 0 0 ... no_of_different_hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "tweets_sequences = tweets_tokenizer.texts_to_sequences(tweets)\n",
    "hashtags_sequences = hashtags_tokenizer.texts_to_sequences(hashtags)\n",
    "tweets_sequences_padded = pad_sequences(tweets_sequences, padding=\"post\", maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "no_of_different_hashtags = len(hashtags_word_index) + 1\n",
    "no_of_hashtags = len(hashtags_sequences)\n",
    "\n",
    "encoded_hashtags = np.zeros((no_of_hashtags, no_of_different_hashtags))\n",
    "\n",
    "for i, hashtags_indices in enumerate(hashtags_sequences):\n",
    "    encoded_hashtags[i][hashtags_indices] = 1\n",
    "\n",
    "\n",
    "encoded_hashtags = np.array(encoded_hashtags)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = 0.8\n",
    "training_tweets_count = int(0.8 * len(tweets_sequences_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tweets_sequences_padded[0:training_tweets_count]\n",
    "train_labels = encoded_hashtags[0:training_tweets_count]\n",
    "test_data = tweets_sequences_padded[training_tweets_count:]\n",
    "test_labels = encoded_hashtags[training_tweets_count:]\n",
    "\n",
    "print(f'we have {len(train_data)} tweets for training and {len(test_data)} for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "embedding_dimensions = 300\n",
    "lstm_units = 128\n",
    "dropout_value = 0.2\n",
    "conv_filters = 64\n",
    "conv_kernel_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 10, 300)           3110400   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 10, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256)              439296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2180)              560260    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,109,956\n",
      "Trainable params: 999,556\n",
      "Non-trainable params: 3,110,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers\n",
    "\n",
    "no_of_tweets_words = len(tweets_word_index) + 1\n",
    "\n",
    "hashtag_recommender_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(no_of_tweets_words, embedding_dimensions,\n",
    "                               input_length=sequence_length,\n",
    "                                 embeddings_initializer=initializers.Constant(embedding_matrix),\n",
    "                                   trainable=False),\n",
    "    # tf.keras.layers.Conv1D(conv_filters, conv_kernel_size, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)),\n",
    "    # tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units)),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Dense(no_of_different_hashtags, activation='softmax')\n",
    "])\n",
    "\n",
    "hashtag_recommender_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "hashtag_recommender_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "174/174 [==============================] - 7s 25ms/step - loss: 8.6174 - accuracy: 0.4107 - val_loss: 8.3941 - val_accuracy: 0.5583\n",
      "Epoch 2/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 8.6253 - accuracy: 0.5642 - val_loss: 10.2885 - val_accuracy: 0.5950\n",
      "Epoch 3/30\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 10.3207 - accuracy: 0.5730 - val_loss: 13.0169 - val_accuracy: 0.5899\n",
      "Epoch 4/30\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 12.0543 - accuracy: 0.5937 - val_loss: 16.8882 - val_accuracy: 0.6115\n",
      "Epoch 5/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 14.1731 - accuracy: 0.5967 - val_loss: 18.4011 - val_accuracy: 0.6000\n",
      "Epoch 6/30\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 15.9646 - accuracy: 0.6021 - val_loss: 19.8425 - val_accuracy: 0.5950\n",
      "Epoch 7/30\n",
      "174/174 [==============================] - 3s 19ms/step - loss: 17.8673 - accuracy: 0.6052 - val_loss: 24.2497 - val_accuracy: 0.5871\n",
      "Epoch 8/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 20.1960 - accuracy: 0.6005 - val_loss: 27.2785 - val_accuracy: 0.5885\n",
      "Epoch 9/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 21.8784 - accuracy: 0.6039 - val_loss: 28.8037 - val_accuracy: 0.5899\n",
      "Epoch 10/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 24.0791 - accuracy: 0.6144 - val_loss: 32.8879 - val_accuracy: 0.5986\n",
      "Epoch 11/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 26.1548 - accuracy: 0.6196 - val_loss: 36.4022 - val_accuracy: 0.5978\n",
      "Epoch 12/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 27.9747 - accuracy: 0.6079 - val_loss: 40.6335 - val_accuracy: 0.5986\n",
      "Epoch 13/30\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 30.5830 - accuracy: 0.6311 - val_loss: 45.3362 - val_accuracy: 0.5957\n",
      "Epoch 14/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 32.7469 - accuracy: 0.6325 - val_loss: 44.5544 - val_accuracy: 0.6129\n",
      "Epoch 15/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 34.5981 - accuracy: 0.6248 - val_loss: 48.6370 - val_accuracy: 0.6050\n",
      "Epoch 16/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 37.2738 - accuracy: 0.6374 - val_loss: 53.2496 - val_accuracy: 0.5935\n",
      "Epoch 17/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 39.0444 - accuracy: 0.6275 - val_loss: 55.1193 - val_accuracy: 0.6094\n",
      "Epoch 18/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 41.8089 - accuracy: 0.6304 - val_loss: 56.8967 - val_accuracy: 0.6180\n",
      "Epoch 19/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 43.0972 - accuracy: 0.6442 - val_loss: 62.7019 - val_accuracy: 0.6151\n",
      "Epoch 20/30\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 45.4354 - accuracy: 0.6493 - val_loss: 65.9980 - val_accuracy: 0.5942\n",
      "Epoch 21/30\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 48.3487 - accuracy: 0.6433 - val_loss: 70.7236 - val_accuracy: 0.6043\n",
      "Epoch 22/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 49.5954 - accuracy: 0.6534 - val_loss: 68.6515 - val_accuracy: 0.6065\n",
      "Epoch 23/30\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 52.4129 - accuracy: 0.6559 - val_loss: 75.1477 - val_accuracy: 0.6043\n",
      "Epoch 24/30\n",
      "174/174 [==============================] - 3s 19ms/step - loss: 54.1269 - accuracy: 0.6608 - val_loss: 73.4879 - val_accuracy: 0.5964\n",
      "Epoch 25/30\n",
      "174/174 [==============================] - 3s 19ms/step - loss: 56.4916 - accuracy: 0.6635 - val_loss: 81.6536 - val_accuracy: 0.6050\n",
      "Epoch 26/30\n",
      "174/174 [==============================] - 3s 19ms/step - loss: 59.0170 - accuracy: 0.6642 - val_loss: 81.1882 - val_accuracy: 0.5957\n",
      "Epoch 27/30\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 61.1834 - accuracy: 0.6649 - val_loss: 86.2573 - val_accuracy: 0.5856\n",
      "Epoch 28/30\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 63.6393 - accuracy: 0.6716 - val_loss: 93.9819 - val_accuracy: 0.5950\n",
      "Epoch 29/30\n",
      "174/174 [==============================] - 4s 23ms/step - loss: 65.7814 - accuracy: 0.6718 - val_loss: 92.3654 - val_accuracy: 0.6065\n",
      "Epoch 30/30\n",
      "174/174 [==============================] - 5s 26ms/step - loss: 68.2786 - accuracy: 0.6687 - val_loss: 99.0354 - val_accuracy: 0.6151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f89099780>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "hashtag_recommender_model.fit(train_data, train_labels, epochs=epochs, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hashtags!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, tweet_tokenizer, hashtag_tokenizer, pad_length, model):\n",
    "    tweet_sequence = tweet_tokenizer.texts_to_sequences([tweet])[0]\n",
    "    padded_tweet_sequence = pad_sequences([tweet_sequence], maxlen=pad_length, padding='post')\n",
    "    prediction = (model.predict(padded_tweet_sequence))\n",
    "    hashtag_indices = np.argsort(prediction, axis=-1)[0][-3:]\n",
    "    return [hashtag_tokenizer.index_word[hashtag_index] for hashtag_index in hashtag_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 656ms/step\n",
      "['barçaatleti', 'nufc', 'newtot']\n"
     ]
    }
   ],
   "source": [
    "print(predict('great match', tweets_tokenizer, hashtags_tokenizer, 10, hashtag_recommender_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
