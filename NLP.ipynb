{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "csv headings: id, created_at, source, original_text, clean_text, favorite_count, retweet_count, hashtags, trend <br>\n",
    "hashtags format: strings with comma separated hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File paths\n",
    "US_tweets_file = './Data/USTweets.csv'\n",
    "UK_tweets_file = './Data/UKTweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "tweets = []\n",
    "hashtags = [] #list of lists of hashtags e.g. hashtags[0] = [\"hashtag1\", \"hashtag2\"]\n",
    "hashtags_strings = [] #list of hashtags string e.g. hashtags[0] = [\"hashtag1, hashtag2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(UK_tweets_file) as data_file:\n",
    "    data = csv.reader(data_file)\n",
    "    next(data) #To skip the headings\n",
    "    for row in data:\n",
    "        tweets.append(row[4])\n",
    "        hashtags.append(row[7].split(\", \"))\n",
    "        hashtags_strings.append(row[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(US_tweets_file) as data_file:\n",
    "#     data = csv.reader(data_file)\n",
    "#     next(data) #To skip the headings\n",
    "#     for row in data:\n",
    "#         tweets.append(row[4])\n",
    "#         hashtags.append(row[7].split(\", \"))\n",
    "#         hashtags_strings.append(row[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tokenizers\n",
    "Will use a specialized tokenizer for the hashtags because we need to encode all the hashtags. It also does not matter if the encoding of the tweets match the encoding of the hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tweets_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tweets_tokenizer.fit_on_texts(tweets)\n",
    "tweets_word_index = tweets_tokenizer.word_index\n",
    "tweets_index_word = tweets_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "hashtags_tokenizer.fit_on_texts(hashtags_strings)\n",
    "hashtags_word_index = hashtags_tokenizer.word_index\n",
    "hashtags_index_word = hashtags_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2154 tweets, \n",
      "the tweets contain 4316 different words.\n",
      "There are 726 different hashtags\n",
      "Here are the tokenized hashtags\n",
      "{'<OOV>': 1, 'facup': 2, 'bhamun': 3, 'nufc': 4, 'mufc': 5, 'newtot': 6, 'emergencyalert': 7, 'thfc': 8, 'spurs': 9, 'coys': 10, 'emiratesfacup': 11, 'bhafc': 12, 'facupsemifinal': 13, 'premierleague': 14, 'tottenham': 15, 'levyout': 16, 'aberdeen': 17, 'rangersfc': 18, 'emergencyalerts': 19, 'newcastle': 20, 'manutd': 21, 'barçaatleti': 22, 'milanlecce': 23, 'sempremilan': 24, 'munbha': 25, 'brighton': 26, 'fpl': 27, 'manchesterunited': 28, 'newcastleunited': 29, 'brimun': 30, 'bbcfootball': 31, 'stgeorgesday': 32, 'davisgarcia': 33, 'bitcoin': 34, 'toon': 35, 'iphone': 36, 'smarty': 37, 'earthday': 38, 'ukraine': 39, 'hwtl': 40, 'enicout': 41, 'nufcfans': 42, 'rangers': 43, 'rfc': 44, 'aberan': 45, 'pl': 46, 'bhamnu': 47, 'amazon': 48, 'ucl': 49, 'kane': 50, 'howaythelads': 51, 'toonarmy': 52, 'ggmu': 53, 'russia': 54, 'whufc': 55, 'glazersout': 56, 'timelessconcert': 57, 'skysports': 58, 'levy': 59, 'motd': 60, 'harrykane': 61, 'kkrvcsk': 62, 'happyjenoday': 63, 'amritpal': 64, 'singh': 65, '鬼滅の刃': 66, 'ジャニーズwestの虹会': 67, 'gerson': 68, 'tottenhamhotspur': 69, 'lnerupdate': 70, 'kingscross': 71, 'bouwhu': 72, 'onthisday': 73, 'stellini': 74, 'football': 75, 'lcfc': 76, '裸聊': 77, 'nifty': 78, 'nifty50': 79, 'sensex': 80, 'stockmarket': 81, 'rcbvrr': 82, 'rcbvsrr': 83, 'nupursharma': 84, 'rcbgogreen': 85, 'tiger3': 86, 'katrinakaif': 87, 'spursy': 88, 'afcb': 89, 'mancity': 90, 'leiwol': 91, 'supersunday': 92, 'bbc': 93, 'influencers': 94, 'gw32': 95, 'standfree': 96, 'aberdeenfc': 97, 'coybig': 98, 'coyr': 99, 'phonecases': 100, 'findyourthing': 101, 'moms': 102, 'bbb': 103, 'willock': 104, 'quote': 105, 'england': 106, 'alarm': 107, 'rwbyv9spoilers': 108, 'marinho': 109, 'ps5': 110, 'modernwarfare2': 111, 'epl': 112, 'magpies': 113, 'fa': 114, 'uk': 115, 'lfc': 116, 'lloris': 117, 'united': 118, 'usiachwenyuma': 119, 'ttid': 120, 'efl': 121, 'abnran': 122, 'spfl': 123, 'dianeabbott': 124, 'abdn': 125, 'ran': 126, 'artwork': 127, 'vodafone': 128, 'shattawale': 129, 'alert': 130, 'bendavies': 131, 'watp': 132, '陪玩': 133, 'orv': 134, 'comedy': 135, 'donotcomply': 136, 'cartoon': 137, 'freenbeckyfanboommnl': 138, 'twitch': 139, 'artistontwitter': 140, 'blackpinkxcoachella': 141, 'russiaislosing': 142, 'shutdownthetbs': 143, 'ebay': 144, 'ipad': 145, 'عيد': 146, 'الفطر': 147, 'المبارك': 148, 'leafsforever': 149, 'ساس': 150, 'newcastlespurs': 151, 'nufcspurs': 152, 'championleague': 153, 'motd2': 154, 'truefaith': 155, 'newtott': 156, 'sorare': 157, 'xklsv': 158, 'learning': 159, 'study': 160, 'afc': 161, 'breavl': 162, 'telegraphfootball': 163, 'toonarmyamalaysia': 164, 'stjamespark': 165, 'worflags': 166, 'sospursy': 167, 'arsenal': 168, 'sttotteringhamsday': 169, 'halland': 170, 'stopairportexpansion': 171, 'podcast': 172, 'chelsea': 173, 'manunited': 174, 'nowplaying': 175, 'family': 176, 'sofascore': 177, 'abe': 178, 'aberfc': 179, 'thedons': 180, 'rcfcabe': 181, 'sakala': 182, 'sahityaakademi': 183, 'iphonecase': 184, 'apple': 185, 'iphone14': 186, 'ai': 187, 'photography': 188, 'ikasells': 189, 'etsy': 190, 'horror': 191, 'twice': 192, 'mitoma': 193, 'itfc': 194, 'rose': 195, 'stgeorge': 196, 'myelopathy': 197, 'rangersfamily': 198, 'afcbournemouth': 199, 'cherries': 200, 'perisic': 201, '灰产': 202, '磕炮': 203, '娇喘': 204, '叫床': 205, 'forster': 206, 'muwomen': 207, 'bhamnj': 208, 'newcastleutd': 209, 'newprofilepic': 210, 'produck': 211, 'duck': 212, 'ducks': 213, 'art': 214, 'illsutration': 215, 'illust': 216, 'blender': 217, 'c4d': 218, 'cinema4d': 219, '3d': 220, 'cg': 221, 'seizonszn': 222, 'ourchangingplanet': 223, 'countryfile': 224, 'colinfromaccounts': 225, 'skysport': 226, 'bigshots': 227, 'vabeachspringlive': 228, 'ugandamusc': 229, 'thechosenone': 230, 'qarmy': 231, 'trackandtrace': 232, 'skipton': 233, 'emergencyalertuk': 234, 'ohio': 235, 'snpout': 236, 'arrestnicolanow': 237, 'polygon': 238, 'sosnhs': 239, 'bullybarclay': 240, 'rcn': 241, 'bma': 242, 'handjuicer': 243, 'juicer': 244, 'mixergrinder': 245, 'handgrinder': 246, 'health': 247, 'healthyjuice': 248, 'housemusic': 249, 'moonbinweloveyou': 250, 'moobin': 251, 'emergencyplanner': 252, 'businessowners': 253, 'businessgrowth': 254, 'grow': 255, 'jesus': 256, 'christian': 257, 'christmas': 258, 'bible': 259, 'trump': 260, 'writerslift': 261, 'book': 262, 'whatemergencyalert': 263, 'gt7': 264, 'gt7purescapes': 265, 'granturismo7': 266, 'granturismo': 267, 'twitchclips': 268, 'svbbank': 269, 'csk': 270, 'rahane': 271, 'msdhoni': 272, 'dhoni': 273, 'shetellmesay': 274, 'abujatwittercommunity': 275, 'thearchers': 276, 'kateking': 277, 'londin': 278, 'kashmir': 279, 'heavens': 280, 'uaapvolleyball': 281, 'viratkohli': 282, 'our': 283, 'birthdayboy': 284, 'jeno': 285, 'atikuwontheelection': 286, 'redefinenigeria': 287, 'کا': 288, 'کرش': 289, 'eid2023': 290, 'زدہ': 291, 'فیصلے': 292, 'eidwithimrankhan': 293, 'swfc': 294, 'goodtimes': 295, 'joelinton': 296, 'nufcawayday': 297, 'talksport': 298, 'geordies': 299, 'nufcvsthfc': 300, 'youtube': 301, 'brazil': 302, 'uefachampionsleague': 303, 'willockinho': 304, 'magicalpass': 305, 'eddiehowesblackandwhitearmy': 306, 'watford': 307, 'casemiro': 308, 'sexism': 309, 'racism': 310, 'islamphobia': 311, 'nufcwomen': 312, 'edinburgh': 313, 'tutorial': 314, 'paydayistimeless': 315, 'onearsenal': 316, 'goonerfamily': 317, 'arsenalfc': 318, 'eidmubarak': 319, 'ghana': 320, 'byebye': 321, 'longisland': 322, 'arssou': 323, 'fullee': 324, 'livnfo': 325, 'cryeve': 326, 'alexanderisak': 327, 'footballwithdme': 328, 'thegatekeeperstvfanzone': 329, 'coyi': 330, 'prayforwizkid': 331, 'bbcfacup': 332, 'mutd': 333, 'brightonhovealbion': 334, 'man': 335, 'torenontour': 336, 'cfc': 337, 'manchester': 338, 'robertodezerbiball': 339, 'jewishgooners': 340, 'electrician': 341, 'hove': 342, 'moulsecoomb': 343, 'caicedo': 344, 'nikujaributu': 345, 'gambingtwitter': 346, 'accountancy': 347, 'livestreaming': 348, 'livestream': 349, 'captain': 350, 'dailymail': 351, 'haiku': 352, 'capitulation': 353, 'embarrassed': 354, 'fmtnews': 355, 'fmtsports': 356, 'callumwilson': 357, 'collapse': 358, 'doubleourefforts': 359, 'dannyrose': 360, 'sustainability': 361, 'newcastletottenham': 362, 'kenya': 363, 'familyacademicslacrosse': 364, 'podcasts': 365, 'ssn': 366, 'antonioconte': 367, 'baftagurucymru': 368, 'pressurestress': 369, 'hotspur': 370, 'yourstotake': 371, 'budweiserpremierleague': 372, 'wxm': 373, 'briman': 374, 'biased': 375, 'motdboycott': 376, 'championsleague': 377, 'competition': 378, 'bayernmunich': 379, 'bhavmnu': 380, 'manu': 381, 'brimau': 382, 'efc': 383, 'rasford': 384, 'devils': 385, 'nbaplayoffs': 386, 'bluecheckmark': 387, 'fortnite': 388, 'characterdesign': 389, 'conceptart': 390, 'kanewilliamson': 391, 'chepaukstadium': 392, 'ipl': 393, 'indianpremierleague': 394, 'staraikelungal': 395, 'lotto': 396, 'battleslam': 397, 'njdvsnyr': 398, 'powerbookiighost': 399, 'farhadsamji': 400, 'yourodds': 401, 'fgc': 402, 'indiegame': 403, 'gamedev': 404, 'zkxyakuza': 405, 'pawson': 406, 'martial': 407, 'unitedforthefans': 408, 'ffscotland': 409, 'loanbhoy': 410, 'hailhail': 411, 'hh': 412, 'hoops': 413, 'celticfamily': 414, 'bhoys': 415, 'pittodrie': 416, 'dons': 417, 'breakingnews': 418, 'fordereport': 419, 'labourfiles': 420, 'antisemitism': 421, 'antiwhite': 422, 'sirsoftie': 423, 'labour': 424, 'odysee': 425, 'doreenlawrence': 426, 'abdngers': 427, 'happychaeyoungday': 428, 'sakalahelpline': 429, 'services': 430, 'thismonththatyear': 431, 'enam': 432, 'medicne': 433, 'doctors': 434, 'funfact': 435, 'nature': 436, 'climate': 437, 'solanke': 438, 'microsoft': 439, 'saudiaramco': 440, 'alphabet': 441, 'success': 442, 'nullvaluesッ': 443, 'ad': 444, 'dealoftheday': 445, 'playstation5': 446, 'lilly': 447, 'privacyoniphone': 448, 'chatgpt': 449, 'chatbot': 450, 'ios': 451, 'zonauang': 452, 'niinisell': 453, 'wallpaper': 454, 'kawaii': 455, 'kmanga': 456, 'アイゼンフリューゲル': 457, 'ベルセルク': 458, 'mobilephotography': 459, 'iphoneography': 460, 'giftdata': 461, 'iphone12': 462, 'iphone13': 463, 'iphonewithoutbox': 464, 'golddigger': 465, '100w': 466, 'usb': 467, 'charger': 468, 'anker': 469, 'nano': 470, 'port': 471, 'fast': 472, 'compact': 473, 'gan': 474, 'macbook': 475, 'iconic': 476, 'spectrummobile': 477, 'iphone15': 478, 'bbtitans': 479, 'blackandwhite': 480, 'blackandwhitephoto': 481, 'blackandwhitephotography': 482, 'architecture': 483, 'dresden': 484, 'defends': 485, 'refused': 486, 'qpr': 487, 'cloud': 488, 'skymobile': 489, 'webinar': 490, 'broadbandcustomers': 491, 'millionsofbroadband': 492, 'mobileandbroadband': 493, 'fujitsu': 494, 'anghami': 495, 'securityalert': 496, 'maalialbum': 497, '트와이스': 498, 'readytobe': 499, '5th': 500, 'world': 501, 'tour': 502, 'fighthype': 503, 'movie': 504, 'april23': 505, 'thelastdrivein': 506, 'davisvsgarcia': 507, 'boxing': 508, 'jimin': 509, 'romekage': 510, 'cashback': 511, 'temu': 512, 'promotion': 513, 'im703peru': 514, 'mvcvmac': 515, 'positivealbionvibes': 516, 'gckinaccra': 517, 'bumblebee': 518, 'ottvoucher': 519, 'alarmtest': 520, 'ballintra': 521, 'countydonegal': 522, 'severealert': 523, 'o2': 524, 'virginmobile': 525, 'music': 526, 'three': 527, 'noalert': 528, 'alerttest': 529, 'threemobile': 530, 'idmobile': 531, 'noemergencyalert': 532, 'pixel7': 533, 'fail': 534, 'ukgov': 535, 'network': 536, 'sscaspirants': 537, 'sscrights': 538, 'london': 539, 'myplmorning': 540, 'samfender': 541, 'stgeorgeandthedragon': 542, 'sikhpatriots': 543, 'sikhheritagemonth': 544, 'george': 545, 'land': 546, 'forces': 547, 'newport': 548, 'fordington': 549, 'dorchester': 550, 'ww1': 551, 'greece': 552, 'ελλάδα': 553, 'aegean': 554, 'mediterranean': 555, 'peloponnese': 556, 'πελοποννησος': 557, 'cpfc': 558, 'hedgewatch': 559, 'hedgewatchers': 560, 'english': 561, 'firsttmaster': 562, 'ukgiftam': 563, 'princessofwales': 564, 'royalfamily': 565, 'realroyals': 566, 'englandrl': 567, 'parade': 568, 'scouts': 569, 'toryfascistdictatorship': 570, 'handsinthetill': 571, 'otd': 572, 'osun': 573, 'getyourgurnon': 574, 'whoseric': 575, 'sonheungmin': 576, 'sonny': 577, '손흥민': 578, 'pusb': 579, 'dezerbi': 580, 'sanchez': 581, 'steele': 582, 'smartnews': 583, 'no': 584, 'ballers': 585, 'whitesox': 586, 'mmopen': 587, 'romerogormaz': 588, 'theunitedstand': 589, 'barcaatleti': 590, 'davinsonsanchez': 591, 'fornals': 592, 'westhamunited': 593, 'citisports': 594, 'shopsmart': 595, 'fh32': 596, 'watch': 597, 'klausschwab': 598, 'apexlegends': 599, 'fishtanklive': 600, '做爱': 601, '高跟': 602, '足控': 603, 'cskvsrr': 604, 'wagner': 605, 'rhoabuja': 606, 'vibegoddess': 607, 'extraqueen': 608, 'bansteve': 609, 'jenlisa': 610, 'jennie': 611, 'lisa': 612, 'weirdnj': 613, 'baymci': 614, 'gers': 615, 'utternonsense': 616, 'cinchprem': 617, 'abdnran': 618, 'twocents': 619, 'inplay': 620, 'woosh': 621, 'scales': 622, 'meantit': 623, 'abrdnran': 624, 'april2023': 625, 'aberdeenlakepark': 626, 'park': 627, 'lake': 628, 'fountain': 629, 'reflection': 630, 'fplcommunity': 631, 'leicester': 632, 'wwfc': 633, 'australiansoccer': 634, 'city': 635, 'optussport': 636, 'leicestermercury': 637, 'foxes': 638, '上海': 639, 'malamute': 640, 'adoptdontshop': 641, '跑分': 642, '偏门': 643, '副业': 644, '支付宝': 645, '项目': 646, '手机赚钱': 647, '快钱': 648, '巴萨': 649, '捕鱼': 650, '呼市': 651, '连云港': 652, 'stelliniout': 653, 'bringbackpoch': 654, '网赚': 655, 'ios梯子': 656, 'creedmoor': 657, 'creedmoorsports': 658, 'reloading': 659, 'hugolloris': 660, 'retire': 661, 'mutantfam': 662, '视频': 663, '幸运5': 664, 'ag真人': 665, '广州': 666, '深圳': 667, '杭州': 668, 'bhavsmnu': 669, 'nft': 670, 'community': 671, 'discord': 672, 'twitter': 673, 'active': 674, 'members': 675, 'wslacademy': 676, 'shite': 677, 'enciso': 678, 'otw': 679, 'supermariobrosmovie': 680, 'bigsixmyarse': 681, 'into1': 682, 'into1就这样长大告别季': 683, 'webridgexbambam': 684, 'tutorkrp': 685, 'nufcthfc': 686, 'jets': 687, 'video': 688, 'gallowgateend': 689, 'degentoonz': 690, 'anime3d': 691, 'toonshader': 692, 'bnpr': 693, 'npr': 694, 'blender3d': 695, 'joongdok': 696, 'omniscientreadersviewpoint': 697, '전독시': 698, 'yoohankim': 699, 'futsal': 700, 'cans': 701, 'うひーメモ': 702, 'ニュース': 703, 'lisachellareturns': 704, 'cars': 705, 'drawings': 706, 'seizongm': 707, 'inspiration': 708, 'famous': 709, 'tinadatta': 710, 'humrahenarahehum': 711, 'breaking': 712, 'dyer': 713, 'onthegear': 714, 'danny': 715, 'hillaryclinton': 716, 'georgesoros': 717, 'awareness': 718, 'training': 719, 'communication': 720, 'absolutelydyer': 721, 'change': 722, 'manchestercity': 723, 'good': 724, 'morning': 725, 'mulive': 726}\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(tweets)} tweets, ')\n",
    "print(f'the tweets contain {len(tweets_tokenizer.word_index)} different words.')\n",
    "print(f'There are {len(hashtags_tokenizer.word_index)} different hashtags')\n",
    "print('Here are the tokenized hashtags')\n",
    "print(hashtags_word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the sequences and pad them and one hot encode the hashtags\n",
    "Will use a binary vector to encode the hashtags to the model can categorize the tweets. e.g. hashtags[0] = [tag1, tag2], and tag1 has encoding of 1 and tag2 has encoding 2, then the binary vector wil be [0 1 1 0 0 ... no_of_different_hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "tweets_sequences = tweets_tokenizer.texts_to_sequences(tweets)\n",
    "hashtags_sequences = hashtags_tokenizer.texts_to_sequences(hashtags)\n",
    "tweets_sequences_padded = pad_sequences(tweets_sequences, padding=\"post\", maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "no_of_different_hashtags = len(hashtags_word_index) + 1\n",
    "no_of_hashtags = len(hashtags_sequences)\n",
    "\n",
    "encoded_hashtags = np.zeros((no_of_hashtags, no_of_different_hashtags))\n",
    "\n",
    "for i, hashtags_indices in enumerate(hashtags_sequences):\n",
    "    encoded_hashtags[i][hashtags_indices] = 1\n",
    "\n",
    "\n",
    "encoded_hashtags = np.array(encoded_hashtags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "embedding_dimensions = 512\n",
    "lstm_units = 100\n",
    "dropout_value = 0.5\n",
    "conv_filters = 64\n",
    "conv_kernel_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 10, 512)           2210304   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 6, 64)             163904    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 6, 64)             0         \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 6, 200)           132000    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 6, 200)            0         \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 200)              240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 727)               146127    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,893,135\n",
      "Trainable params: 2,893,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_of_tweets_words = len(tweets_word_index) + 1\n",
    "\n",
    "hashtag_recommender_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(no_of_tweets_words, embedding_dimensions, input_length=sequence_length),\n",
    "    # tf.keras.layers.Conv1D(conv_filters, conv_kernel_size, activation='relu'),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units)),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Dense(no_of_different_hashtags, activation='softmax')\n",
    "])\n",
    "\n",
    "hashtag_recommender_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "hashtag_recommender_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "68/68 [==============================] - 6s 21ms/step - loss: 8.5964 - accuracy: 0.1806\n",
      "Epoch 2/8\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 7.6109 - accuracy: 0.2925\n",
      "Epoch 3/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 7.9141 - accuracy: 0.3774\n",
      "Epoch 4/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 8.5455 - accuracy: 0.4308\n",
      "Epoch 5/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 9.5820 - accuracy: 0.4508\n",
      "Epoch 6/8\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 10.2500 - accuracy: 0.4675\n",
      "Epoch 7/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 11.1753 - accuracy: 0.4838\n",
      "Epoch 8/8\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 12.0849 - accuracy: 0.4782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231e0faece0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 8\n",
    "hashtag_recommender_model.fit(tweets_sequences_padded, encoded_hashtags, epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hashtags!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, tweet_tokenizer, hashtag_tokenizer, pad_length, model):\n",
    "    tweet_sequence = tweet_tokenizer.texts_to_sequences([tweet])[0]\n",
    "    padded_tweet_sequence = pad_sequences([tweet_sequence], maxlen=pad_length, padding='post')\n",
    "    prediction = (model.predict(padded_tweet_sequence))\n",
    "    hashtag_index = np.argmax(prediction, axis=-1)[0]\n",
    "    return hashtag_tokenizer.index_word[hashtag_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "levyout\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"oh my god\", tweets_tokenizer, hashtags_tokenizer, 10, hashtag_recommender_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
