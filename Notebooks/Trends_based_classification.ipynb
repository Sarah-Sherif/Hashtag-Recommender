{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends-based recommendation\n",
    "In this notebook we will classify the tweets into trends and these trends will help us decide which hashtags to recommend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from files_reader import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_trends = []\n",
    "tweets = []\n",
    "trends = []\n",
    "\n",
    "# tweets_and_trends += (FilesReader.read_file(UK_tweets_file))\n",
    "# tweets_and_trends += (FilesReader.read_file(US_tweets_file))\n",
    "# tweets_and_trends += (FilesReader.read_file(AUS_tweets_file))\n",
    "# tweets_and_trends += (FilesReader.read_file(IR_tweets_file))\n",
    "# tweets_and_trends += (FilesReader.read_file(CAN_tweets_file))\n",
    "tweets_and_trends += (FilesReader.read_file(new_US_file))\n",
    "tweets_and_trends += (FilesReader.read_file(new_UK_file))\n",
    "tweets_and_trends += (FilesReader.read_file(new_AUS_file))\n",
    "tweets_and_trends += (FilesReader.read_file(new_CAN_file))\n",
    "tweets_and_trends += (FilesReader.read_file(new_IR_file))\n",
    "# tweets_and_trends += (FilesReader.read_file(new_SINGA_file))   \n",
    "# tweets_and_trends += (FilesReader.read_file(new_SA_file))\n",
    "\n",
    "random.shuffle(tweets_and_trends)\n",
    "\n",
    "tweets, trends = FilesReader.split_tweets_and_trends(tweets_and_trends)\n",
    "print(f\"We have {len(tweets)} tweets.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming the words\n",
    "i.e turning words like playing, plays played to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# def stem_tweet(tweet):\n",
    "#     ps = PorterStemmer()\n",
    "#     new_tweet = \"\"\n",
    "#     for word in tweet.split(' '):\n",
    "#         new_tweet += ps.stem(word) + ' '\n",
    "#     return new_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"before: {tweets[:3]}\")\n",
    "# for i, tweet in enumerate(tweets):\n",
    "#     tweets[i] = stem_tweet(tweet)\n",
    "\n",
    "# print(f\"after: {tweets[:3]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tweets_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tweets_tokenizer.fit_on_texts(tweets)\n",
    "tweets_word_index = tweets_tokenizer.word_index\n",
    "tweets_index_word = tweets_tokenizer.index_word\n",
    "\n",
    "print(f\"We have {len(tweets_word_index)} different words\")\n",
    "print(tweets_word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "sequence_length = 15\n",
    "\n",
    "tweets_sequences = tweets_tokenizer.texts_to_sequences(tweets)\n",
    "tweets_sequences_padded = pad_sequences(tweets_sequences, padding=\"post\", maxlen=sequence_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the trends to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_map = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for trend in trends:\n",
    "    if not (trend in trends_map):\n",
    "        trends_map[trend] = counter\n",
    "        counter += 1\n",
    "\n",
    "no_of_trends = len(trends_map)\n",
    "inv_trends_map = {v: k for k, v in trends_map.items()}\n",
    "print(f\"We have {no_of_trends} different trends\")\n",
    "print(trends_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the trends sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_sequences = [trends_map[trend] for trend in trends]\n",
    "print(trends_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "encoded_trends = to_categorical(trends_sequences)\n",
    "print(encoded_trends.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings.embeddings_matrix import get_embeddings_matrix\n",
    "\n",
    "embeddings_index_path = \"./Embeddings/embeddings_index_object.pkl\"\n",
    "embeddings_matrix, hits, misses = get_embeddings_matrix(tweets_word_index, embeddings_index_path)\n",
    "\n",
    "print(f\"Hits: {hits}, Misses: {misses}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = 0.8\n",
    "training_tweets_count = int(0.8 * len(tweets_sequences_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tweets_sequences_padded[0:training_tweets_count]\n",
    "train_labels = encoded_trends[0:training_tweets_count]\n",
    "test_data = tweets_sequences_padded[training_tweets_count:]\n",
    "test_labels = encoded_trends[training_tweets_count:]\n",
    "\n",
    "print(f'we have {len(train_data)} tweets for training and {len(test_data)} for testing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# import keras\n",
    "# from keras import initializers\n",
    "# import tensorflow as tf\n",
    "\n",
    "# no_of_tweets_words = len(tweets_word_index) + 1\n",
    "# embedding_dimensions = 300\n",
    "\n",
    "# def model_builder(hp):\n",
    "#     model = keras.Sequential()\n",
    "\n",
    "#     hp_conv_filters = hp.Int('conv_filters', min_value=32, max_value=256, step=16)\n",
    "#     hp_kernel_size = hp.Int('conv_kernel_size', min_value=2, max_value=5, step=1)\n",
    "#     hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.6, step=0.1)\n",
    "#     hp_lstm = hp.Int('lstm_units', min_value=32, max_value=256, step=16)\n",
    "\n",
    "#     model.add(keras.layers.Embedding(\n",
    "#         no_of_tweets_words,\n",
    "#         embedding_dimensions,\n",
    "#         input_length=sequence_length,\n",
    "#         embeddings_initializer=initializers.Constant(embeddings_matrix),\n",
    "#         trainable=True\n",
    "#     ))\n",
    "#     model.add(keras.layers.Conv1D(hp_conv_filters, hp_kernel_size, padding='same'))\n",
    "#     model.add(keras.layers.Dropout(hp_dropout))\n",
    "#     model.add(keras.layers.Bidirectional(keras.layers.LSTM(hp_lstm)))\n",
    "#     model.add(keras.layers.Dropout(hp_dropout))\n",
    "#     model.add(keras.layers.Dense(no_of_trends))\n",
    "\n",
    "#     # Tune the learning rate for the optimizer\n",
    "#     # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "#                   loss=keras.losses.CategoricalCrossentropy(\n",
    "#                       from_logits=True),\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = kt.Hyperband(model_builder,\n",
    "#                      objective='val_accuracy',\n",
    "#                      max_epochs=6,\n",
    "#                      factor=3,\n",
    "#                      directory='parameters_tuning',\n",
    "#                      project_name='trends_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(train_data, train_labels, epochs=50, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. Here are the optimal configurations:\n",
    "#     conv_filters: {best_hps.get('conv_filters')}\n",
    "#     conv_kernel_size: {best_hps.get('conv_kernel_size')}\n",
    "#     lstm_units: {best_hps.get('lstm_units')}\n",
    "#     dropout: {best_hps.get('dropout')}\n",
    "#     learning_rate: {best_hps.get('learning_rate')}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trends_classifier = tuner.hypermodel.build(best_hps)\n",
    "# history = trends_classifier.fit(train_data, train_labels, epochs=20, validation_data=(test_data, test_labels))\n",
    "\n",
    "# val_acc_per_epoch = history.history['val_accuracy']\n",
    "# best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "# print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trends_classifier = tuner.hypermodel.build(best_hps)\n",
    "# trends_classifier.fit(train_data, train_labels, epochs=best_epoch, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "After tuning the hyper-parameters, here are the optimal configurations: <br>\n",
    "    conv_filters: 224 <br>\n",
    "    conv_kernel_size: 2<br>\n",
    "    lstm_units: 144<br>\n",
    "    dropout: 0.1<br>\n",
    "    learning_rate: 0.001<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "embedding_dimensions = 300\n",
    "lstm_units = 144\n",
    "dropout_value = 0.1\n",
    "conv_filters = 224\n",
    "conv_kernel_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "import tensorflow as tf\n",
    "\n",
    "no_of_tweets_words = len(tweets_word_index) + 1\n",
    "\n",
    "trends_classifier = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        no_of_tweets_words,\n",
    "        embedding_dimensions,\n",
    "        input_length=sequence_length,\n",
    "        embeddings_initializer=initializers.Constant(embeddings_matrix),\n",
    "        trainable=True\n",
    "    ),\n",
    "    tf.keras.layers.Conv1D(conv_filters, conv_kernel_size),\n",
    "    tf.keras.layers.AveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units)),\n",
    "    tf.keras.layers.Dropout(dropout_value),\n",
    "    tf.keras.layers.Dense(no_of_trends, activation='softmax')\n",
    "])\n",
    "\n",
    "trends_classifier.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "trends_classifier.summary()\n",
    "\n",
    "# trends_classifier = tf.keras.models.load_model(\"trends_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "trends_classifier.fit(train_data, train_labels, epochs=epochs, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "trends_classifier.save(\"./trends_classifier/trends_classifier_model.h5\")\n",
    "with open('./trends_classifier/inv_trends_map.pkl', 'wb') as output:\n",
    "    pickle.dump(inv_trends_map, output)\n",
    "with open('./trends_classifier/tweet_tokenizer.pkl', 'wb') as output:\n",
    "    pickle.dump(tweets_tokenizer, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(tweet, tweet_tokenizer, trends_map, inv_trends_map, pad_length, model):\n",
    "    tweet_sequence = tweet_tokenizer.texts_to_sequences([tweet])[0]\n",
    "    padded_tweet_sequence = pad_sequences([tweet_sequence], maxlen=pad_length, padding='post')\n",
    "    prediction = (model.predict(padded_tweet_sequence))\n",
    "    trends_indices = np.argsort(prediction, axis=-1)[0][-3:]\n",
    "    return [inv_trends_map[trend_index] for trend_index in trends_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"happy may day.\"\n",
    "\n",
    "print(predict(tweet, tweets_tokenizer, trends_map, inv_trends_map, sequence_length, trends_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
